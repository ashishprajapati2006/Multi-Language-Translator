# ğŸŒ Multi-Language Translator

A complete machine learning project for translating between 10 languages using both custom Seq2Seq (LSTM) and pre-trained mBART models, with Flask and Streamlit web interfaces.

## ğŸŒ Supported Languages

- ğŸ‡¬ğŸ‡§ English (en_US)
- ğŸ‡©ğŸ‡ª German (de_DE)
- ğŸ‡®ğŸ‡³ Hindi (hi_IN)
- ğŸ‡ªğŸ‡¸ Spanish (es_ES)
- ğŸ‡«ğŸ‡· French (fr_FR)
- ğŸ‡®ğŸ‡¹ Italian (it_IT)
- ğŸ‡¸ğŸ‡¦ Arabic (ar_SA)
- ğŸ‡³ğŸ‡± Dutch (nl_NL)
- ğŸ‡¯ğŸ‡µ Japanese (ja_JP)
- ğŸ‡µğŸ‡¹ Portuguese (pt_PT)

## ğŸ“ Project Structure

```
Language translator/
â”œâ”€â”€ app.py                      # Flask web app with dual models support
â”œâ”€â”€ app2.py                     # Streamlit interface for translation
â”œâ”€â”€ all_translate.ipynb         # Complete translation workflow notebook
â”œâ”€â”€ transtate.ipynb             # Model training notebook
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore configuration
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mbart_multilingual/     # Pre-trained mBART model files
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â””â”€â”€ tokenizer.json
â””â”€â”€ templates/
    â””â”€â”€ index3.html             # Flask web interface
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run Streamlit App (Recommended)

```bash
streamlit run app2.py
```
Access at: http://localhost:8501

### 3. Run Flask App (Alternative)

```bash
python app.py
```
Access at: http://localhost:5000

## ğŸ“Š Model Architecture

### Custom Seq2Seq Model
- **Encoder**: 2-layer LSTM with embedding and dropout
- **Decoder**: 2-layer LSTM with fully connected output layer
- **Language Support**: **English (en_US) â†’ Spanish (es_ES) only**
- **Hyperparameters**:
  - Embedding size: 256
  - Hidden size: 512
  - Layers: 2
  - Dropout: 0.5
  - Optimizer: Adam (learning rate: 0.001)
  - Batch size: 32

### Pre-trained mBART Model
- Facebook's `mbart-large-50-many-to-many-mmt`
- Supports 50+ languages
- Beam search with num_beams=4
- Max length: 128 tokens

## ğŸ¯ Features

### Streamlit App (app2.py)
- âœ… Clean, modern UI with language selection
- âœ… Swap languages button for convenience
- âœ… Model selection (Custom or mBART)
- âœ… Real-time translation
- âœ… Cached model loading for performance
- âœ… Error handling and validation
- âš ï¸ **Custom model restricted to English â†’ Spanish translation only**

### Flask App (app.py)
- âœ… Web interface with responsive design
- âœ… Dual endpoints: `/translate_custom` and `/translate_mbart`
- âœ… JSON API support
- âœ… Character counter
- âœ… Loading indicators

## ğŸ“– Training Custom Models

Open `transtate.ipynb` in Jupyter Notebook or VS Code:

1. Load dataset from Hugging Face
2. Exploratory Data Analysis (EDA)
3. Text preprocessing and normalization
4. Build source and target vocabularies
5. Create and train Seq2Seq model
6. Evaluate and test translations
7. Save model as pickle file

To train for different language pairs, modify:
```python
SRC_LANG = 'en_US'  # Change source language
TGT_LANG = 'es_ES'  # Change target language
```

Trained models are saved as: `models/translator_{src_lang}_to_{tgt_lang}.pkl`

## ğŸ”§ API Endpoints (Flask)

### POST /translate_custom
Translate using custom Seq2Seq model

**Request:**
```json
{
  "text": "Hello, how are you?",
  "source_lang": "en_US",
  "target_lang": "es_ES"
}
```

**Response:**
```json
{
  "original": "Hello, how are you?",
  "translation": "hola como estas",
  "source_lang": "English",
  "target_lang": "Spanish"
}
```

### POST /translate_mbart
Translate using mBART model

Same request/response format as above.

## ğŸ“š Dataset

Uses `Amani27/massive_translation_dataset` from Hugging Face, containing:
- Parallel translations across multiple languages
- High-quality translation pairs
- Diverse domains and topics

## ğŸ¨ Customization

### Adjust Model Hyperparameters
Edit in `transtate.ipynb`:
```python
EMBEDDING_SIZE = 256
HIDDEN_SIZE = 512
NUM_LAYERS = 2
DROPOUT = 0.5
LEARNING_RATE = 0.001
BATCH_SIZE = 32
NUM_EPOCHS = 10
```

## ğŸ“¦ Dependencies

- **Deep Learning**: torch, transformers
- **Web Framework**: streamlit
- **Data Processing**: numpy, pandas, datasets, nltk, tqdm
- **Visualization**: matplotlib, seaborn

## âš¡ Performance Tips

1. Use GPU if available (automatically detected)
2. Increase `NUM_EPOCHS` for better custom model accuracy
3. Adjust `BATCH_SIZE` based on available memory
4. Use mBART for faster inference (pre-trained)
5. Use custom model for domain-specific translations

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| Model not found error | Train the model first using `transtate.ipynb` |
| Out of memory | Reduce `BATCH_SIZE` or `HIDDEN_SIZE` |
| Poor translation quality | Increase `NUM_EPOCHS` or use mBART model |
| SentencePiece missing | `pip install sentencepiece` |
| Streamlit not loading models | Clear Streamlit cache: `streamlit cache clear` |

## ğŸ‘¨â€ğŸ’» Built With

- **PyTorch** - Deep learning framework
- **Transformers** - Pre-trained models (mBART)
- **Streamlit** - Web interface
- **Flask** - Alternative web framework
- **Pandas & NumPy** - Data processing
- **Matplotlib & Seaborn** - Visualization

## ğŸ“ Notes

- Custom models are trained using the Seq2Seq architecture with LSTM
- mBART provides faster inference and better multilingual support
- All models are cached after first load for improved performance
- Text is normalized before translation (lowercase, accent removal)
