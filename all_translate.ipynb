{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d50fd42",
   "metadata": {},
   "source": [
    "# Multilingual Translator (mBART-50)\n",
    "A compact notebook to fine-tune a single multilingual model that can translate between all 10 languages.\n",
    "\n",
    "This setup is CPU-friendly, but it keeps the training tiny. Increase `SAMPLE_ROWS` or `NUM_EPOCHS` if you have more time or a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a432025",
   "metadata": {},
   "source": [
    "## Step 1: Install dependencies\n",
    "Run this once if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6638900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# If you already have these, you can skip this cell.\n",
    "# %pip is supported in notebooks\n",
    "%pip install -q transformers datasets sentencepiece accelerate sacrebleu protobuf tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ddead",
   "metadata": {},
   "source": [
    "## Step 2: Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2812f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    "    )\n",
    "\n",
    "MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "SAMPLE_ROWS = 500  # smaller subset for CPU\n",
    "MAX_TRAIN_PAIRS = 500\n",
    "MAX_LEN = 48\n",
    "NUM_EPOCHS = 1\n",
    "MAX_STEPS = 100  # hard cap on steps\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "SEED = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(SEED)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01f601",
   "metadata": {},
   "source": [
    "## Step 3: Load dataset and build multilingual pairs\n",
    "Each example randomly picks a source language and a different target language. This gives coverage across all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d304b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs: 500\n"
     ]
    }
   ],
   "source": [
    "languages = [\"en_US\", \"de_DE\", \"hi_IN\", \"es_ES\", \"fr_FR\", \"it_IT\", \"ar_SA\", \"nl_NL\", \"ja_JP\", \"pt_PT\"]\n",
    "\n",
    "# Map dataset codes to mBART-50 language codes\n",
    "mbart_lang_map = {\n",
    "    \"en_US\": \"en_XX\",\n",
    "    \"de_DE\": \"de_DE\",\n",
    "    \"hi_IN\": \"hi_IN\",\n",
    "    \"es_ES\": \"es_XX\",\n",
    "    \"fr_FR\": \"fr_XX\",\n",
    "    \"it_IT\": \"it_IT\",\n",
    "    \"ar_SA\": \"ar_AR\",\n",
    "    \"nl_NL\": \"nl_XX\",\n",
    "    \"ja_JP\": \"ja_XX\",\n",
    "    \"pt_PT\": \"pt_XX\",\n",
    "}\n",
    "\n",
    "ds = load_dataset(\"Amani27/massive_translation_dataset\")\n",
    "train_df = pd.DataFrame(ds[\"train\"])\n",
    "\n",
    "if SAMPLE_ROWS < len(train_df):\n",
    "    train_df = train_df.sample(SAMPLE_ROWS, random_state=SEED)\n",
    "\n",
    "def build_pairs(df, seed=SEED):\n",
    "    rnd = random.Random(seed)\n",
    "    pairs = []\n",
    "    for _, row in df.iterrows():\n",
    "        src_lang = rnd.choice(languages)\n",
    "        tgt_lang = rnd.choice([l for l in languages if l != src_lang])\n",
    "        src_text = str(row[src_lang])\n",
    "        tgt_text = str(row[tgt_lang])\n",
    "        if not src_text or not tgt_text:\n",
    "            continue\n",
    "        pairs.append({\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "            \"src_lang\": mbart_lang_map[src_lang],\n",
    "            \"tgt_lang\": mbart_lang_map[tgt_lang],\n",
    "        })\n",
    "    return pairs\n",
    "\n",
    "train_pairs = build_pairs(train_df)\n",
    "if MAX_TRAIN_PAIRS < len(train_pairs):\n",
    "    train_pairs = train_pairs[:MAX_TRAIN_PAIRS]\n",
    "train_dataset = Dataset.from_list(train_pairs)\n",
    "print(f\"Training pairs: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609c822",
   "metadata": {},
   "source": [
    "## Step 4: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57efe4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2051.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "def preprocess(example):\n",
    "    tokenizer.src_lang = example[\"src_lang\"]\n",
    "    tokenizer.tgt_lang = example[\"tgt_lang\"]\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"src_text\"],\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=example[\"tgt_text\"],\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=MODEL_NAME)\n",
    "print(\"Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fc92d",
   "metadata": {},
   "source": [
    "## Step 5: Train (lightweight fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d316b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 516/516 [00:01<00:00, 515.55it/s, Materializing param=model.shared.weight]                                   \n",
      "c:\\Users\\ashis\\Desktop\\ML Projects\\Language translator\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.569272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.356655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.476489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.810697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.864709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:20<00:00, 20.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: models/mbart_multilingual\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "output_dir = \"models/mbart_multilingual\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    predict_with_generate=False,\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552cf4bf",
   "metadata": {},
   "source": [
    "## Step 6: Translate between any languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6866cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 516/516 [00:01<00:00, 486.46it/s, Materializing param=model.shared.weight]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_US -> es_ES: Hola, como está?\n",
      "en_US -> hi_IN: मैं प्रोग्रामिंग को पसंद करता हूँ\n",
      "en_US -> fr_FR: Le temps est beau aujourd'hui.\n",
      "en_US -> ja_JP: 何時ですか?\n",
      "en_US -> de_DE: Vielen Dank.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "\n",
    "# Load saved model/tokenizer if available; otherwise use the base model\n",
    "if os.path.isdir(output_dir) and (\n",
    "    os.path.exists(os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    or os.path.exists(os.path.join(output_dir, \"model.safetensors\"))\n",
    "    ):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(output_dir).to(device)\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(output_dir)\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def translate(text, src_lang, tgt_lang, max_length=64):\n",
    "    src_code = mbart_lang_map[src_lang]\n",
    "    tgt_code = mbart_lang_map[tgt_lang]\n",
    "\n",
    "    tokenizer.src_lang = src_code\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "    # Get the token id for the target language\n",
    "    forced_bos = tokenizer.convert_tokens_to_ids(tgt_code)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **encoded,\n",
    "            forced_bos_token_id=forced_bos,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    return tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "examples = [\n",
    "    (\"Hello, how are you?\", \"en_US\", \"es_ES\"),\n",
    "    (\"I love programming.\", \"en_US\", \"hi_IN\"),\n",
    "    (\"The weather is beautiful today.\", \"en_US\", \"fr_FR\"),\n",
    "    (\"What time is it?\", \"en_US\", \"ja_JP\"),\n",
    "    (\"Thank you very much.\", \"en_US\", \"de_DE\"),\n",
    "]\n",
    "\n",
    "for text, src, tgt in examples:\n",
    "    print(f\"{src} -> {tgt}: {translate(text, src, tgt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475248b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
